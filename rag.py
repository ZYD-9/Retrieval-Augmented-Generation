# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nIC0ayUhKxtq-SENZTALTa6igfZkAMNV
"""

!pip install --q unstructured langchain

!pip install -- "unstructured [all-docs]"

!pip install -U langchain-community

!pip install --q chromadb

from langchain.document_loaders import UnstructuredURLLoader
from langchain.document_loaders import OnlinePDFLoader
from langchain.document_loaders import UnstructuredPDFLoader

"""##Load PDF or Document Here"""

pdf_path = "/content/drive/MyDrive/research.pdf"

if pdf_path:
  loader = UnstructuredPDFLoader(file_path = pdf_path)
  data = loader.load()
else:
  print("No PDF file found please upload")

data[0].page_content

!pip install colab-xterm #https://pypi.org/project/colab-xterm/

# Commented out IPython magic to ensure Python compatibility.
# %load_ext colabxterm

# Commented out IPython magic to ensure Python compatibility.
# %xterm

!ollama pull nomic-embed-text

!ollama list

from langchain.embeddings import OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
chunks  = text_splitter.split_documents(data)

"""##  Vector Database(Chroma)"""

vector_db = Chroma.from_documents(
    documents=chunks,
    embedding=OllamaEmbeddings(model="nomic-embed-text",show_progress=True),
    persist_directory="db"
)

"""## Retrieval Generation Task"""

from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.chat_models import ChatOllama
from langchain_core.runnables import RunnablePassthrough
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.prompts import ChatPromptTemplate

!ollama pull mistral

!ollama pull llama2

my_model = 'mistral'
llm = ChatOllama(model_name=my_model)

QUERY_PROMPT_TEMPLATE = PromptTemplate(
    input_variables=["question"],
    template= """ You are an AI language model assistant your task is to generate another 3 relevant questions based on the input.
                Only answer the question that are related to the document
               questions based on the input and the context.Question: {question} References:""",
)

retriever = MultiQueryRetriever.from_llm(
    vector_db.as_retriever(),
    llm,
    prompt = QUERY_PROMPT_TEMPLATE

)

template = """ Answer the following questions using the context below:
{context}
Question: {question}
"""
prompt =  ChatPromptTemplate.from_template(template)

chain = (
    {"context":retriever,"question":RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

chain.invoke(input(""))

chain.invoke(input(""))

chain.invoke(input(""))